{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNPSR1x5UoQkFWdWYYuBbmW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/odeandialamsyah/nlp_zero_to_hero/blob/main/NLP_Zero_to_Hero_Implementas_Word_Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Meskipun melatih model Word Embeddings (seperti Word2Vec atau GloVe) dari nol memerlukan korpus teks yang sangat besar dan daya komputasi yang signifikan, kabar baiknya adalah kita bisa memanfaatkan model pre-trained yang sudah tersedia! Model-model ini dilatih pada miliaran kata dan telah belajar representasi makna yang sangat kaya.\n",
        "\n",
        "Kita akan menggunakan library gensim untuk memuat dan berinteraksi dengan model Word2Vec, dan juga menunjukkan cara dasar untuk melatih model Anda sendiri jika Anda memiliki korpus yang lebih kecil."
      ],
      "metadata": {
        "id": "4lNSpUybDtG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dC-DSn5qEvJB",
        "outputId": "74ddc4fd-9614-49c3-95e8-51a886877b7e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.3.0.post1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WB88xfeYDRuo",
        "outputId": "37c48954-bc03-4206-958c-18fafa0ade27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mengunduh model 'glove-wiki-gigaword-50'...\n",
            "[==================================================] 100.0% 66.0/66.0MB downloaded\n",
            "Model berhasil diunduh dan dimuat!\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Unduh model Word2Vec yang lebih kecil. Ada banyak pilihan di gensim.downloader.\n",
        "# 'word2vec-google-news-300' adalah model besar (sekitar 3.6GB).\n",
        "# Mari kita gunakan yang lebih kecil untuk demo, misalnya 'glove-wiki-gigaword-50' (sekitar 60MB).\n",
        "# Ini akan mengunduh model jika belum ada.\n",
        "try:\n",
        "    print(\"Mengunduh model 'glove-wiki-gigaword-50'...\")\n",
        "    model_glove = api.load(\"glove-wiki-gigaword-50\")\n",
        "    print(\"Model berhasil diunduh dan dimuat!\")\n",
        "except Exception as e:\n",
        "    print(f\"Gagal mengunduh atau memuat model: {e}\")\n",
        "    print(\"Anda mungkin tidak memiliki koneksi internet atau model terlalu besar. \"\n",
        "          \"Akan menggunakan model dummy untuk demonstrasi.\")\n",
        "    model_glove = None # Set to None if download fails"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jika model_glove berhasil dimuat"
      ],
      "metadata": {
        "id": "5AkqD3zeEZVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if model_glove:\n",
        "    print(\"\\n--- Menggunakan Model Word Embeddings (GloVe) ---\")\n",
        "\n",
        "    # Mendapatkan vektor untuk sebuah kata\n",
        "    kata_kucing = \"cat\"\n",
        "    if kata_kucing in model_glove:\n",
        "        vector_kucing = model_glove[kata_kucing]\n",
        "        print(f\"Vektor untuk '{kata_kucing}': {vector_kucing[:10]}...\") # Tampilkan 10 dimensi pertama\n",
        "        print(f\"Ukuran vektor: {len(vector_kucing)} dimensi\")\n",
        "    else:\n",
        "        print(f\"Kata '{kata_kucing}' tidak ditemukan dalam kamus model.\")\n",
        "\n",
        "    # Mencari kata-kata paling mirip (most similar)\n",
        "    kata_target = \"king\"\n",
        "    print(f\"\\nKata-kata paling mirip dengan '{kata_target}':\")\n",
        "    if kata_target in model_glove:\n",
        "        similar_words = model_glove.most_similar(kata_target)\n",
        "        for word, similarity in similar_words:\n",
        "            print(f\"- {word}: {similarity:.4f}\")\n",
        "    else:\n",
        "        print(f\"Kata '{kata_target}' tidak ditemukan dalam kamus model.\")\n",
        "\n",
        "    # Analogi kata (seperti Raja - Pria + Wanita = Ratu)\n",
        "    print(\"\\n--- Analogi Kata (King - Man + Woman = Queen) ---\")\n",
        "    positive = ['king', 'woman']\n",
        "    negative = ['man']\n",
        "    try:\n",
        "        result = model_glove.most_similar(positive=positive, negative=negative)\n",
        "        for word, similarity in result:\n",
        "            print(f\"- {word}: {similarity:.4f}\")\n",
        "    except KeyError as e:\n",
        "        print(f\"Salah satu kata untuk analogi tidak ditemukan: {e}\")\n",
        "else:\n",
        "    print(\"\\nModel pre-trained tidak tersedia. Tidak dapat menjalankan contoh ini.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJxbUwTIER87",
        "outputId": "0467f922-2280-4bcb-d536-ee65ae61493e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Menggunakan Model Word Embeddings (GloVe) ---\n",
            "Vektor untuk 'cat': [ 0.45281  -0.50108  -0.53714  -0.015697  0.22191   0.54602  -0.67301\n",
            " -0.6891    0.63493  -0.19726 ]...\n",
            "Ukuran vektor: 50 dimensi\n",
            "\n",
            "Kata-kata paling mirip dengan 'king':\n",
            "- prince: 0.8236\n",
            "- queen: 0.7839\n",
            "- ii: 0.7746\n",
            "- emperor: 0.7736\n",
            "- son: 0.7667\n",
            "- uncle: 0.7627\n",
            "- kingdom: 0.7542\n",
            "- throne: 0.7540\n",
            "- brother: 0.7492\n",
            "- ruler: 0.7434\n",
            "\n",
            "--- Analogi Kata (King - Man + Woman = Queen) ---\n",
            "- queen: 0.8524\n",
            "- throne: 0.7664\n",
            "- prince: 0.7592\n",
            "- daughter: 0.7474\n",
            "- elizabeth: 0.7460\n",
            "- princess: 0.7425\n",
            "- kingdom: 0.7337\n",
            "- monarch: 0.7214\n",
            "- eldest: 0.7185\n",
            "- widow: 0.7099\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Penjelasan Kode Menggunakan Model Pre-trained:\n",
        "1. import gensim.downloader as api: Mengimpor modul downloader dari gensim yang memudahkan kita mengunduh model pre-trained yang umum.\n",
        "\n",
        "2. model_glove = api.load(\"glove-wiki-gigaword-50\"): Ini adalah baris penting untuk mengunduh dan memuat model GloVe. glove-wiki-gigaword-50 berarti model GloVe yang dilatih pada Wikipedia dan Gigaword, dengan vektor berukuran 50 dimensi. Ada banyak model lain yang bisa dipilih. Jika model belum ada di komputer Anda, ia akan diunduh.\n",
        "\n",
        "3. vector_kucing = model_glove[kata_kucing]: Setelah model dimuat, Anda bisa mengakses vektor representasi untuk sebuah kata hanya dengan menggunakan kata tersebut sebagai kunci indeks (seperti kamus Python).\n",
        "\n",
        "4. model_glove.most_similar(kata_target): Fungsi ini mencari kata-kata lain dalam kamus model yang memiliki vektor paling \"dekat\" (paling mirip) dengan vektor kata_target. Ini menunjukkan konsep bahwa kata-kata dengan makna serupa akan berdekatan di ruang vektor.\n",
        "\n",
        "5. model_glove.most_similar(positive=['king', 'woman'], negative=['man']): Ini adalah demonstrasi yang menarik dari sifat aljabar linear pada Word Embeddings. Kita mencari kata yang paling mirip dengan king dan woman, setelah \"mengurangi\" (negative) man. Secara intuitif, ini mencari V_kingâˆ’V_man+V_woman. Hasil idealnya adalah queen. Ini menunjukkan bagaimana Word Embeddings menangkap hubungan semantik."
      ],
      "metadata": {
        "id": "tx0SZZ4XFnnI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Melatih Model Word2Vec Sendiri\n",
        "\n",
        "Jika Anda ingin melatih model Word2Vec Anda sendiri dari korpus yang lebih kecil, gensim juga menyediakannya. Ini bagus untuk memahami bagaimana prosesnya bekerja."
      ],
      "metadata": {
        "id": "GSQFIHc4F2Tf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Contoh korpus (daftar kalimat yang sudah ditokenisasi)\n",
        "# Setiap \"kalimat\" adalah daftar kata-kata. Ini adalah hasil dari proses tokenisasi Anda.\n",
        "korpus_tokenized = [\n",
        "    ['saya', 'suka', 'makan', 'apel'],\n",
        "    ['saya', 'suka', 'pisang', 'dan', 'apel'],\n",
        "    ['kucing', 'suka', 'ikan'],\n",
        "    ['anjing', 'suka', 'bermain'],\n",
        "    ['raja', 'adalah', 'pria', 'penguasa'],\n",
        "    ['ratu', 'adalah', 'wanita', 'penguasa'],\n",
        "    ['pria', 'bekerja'],\n",
        "    ['wanita', 'belajar']\n",
        "]"
      ],
      "metadata": {
        "id": "pN7wJ2TOFviF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Melatih Model Word2Vec Sendiri ---\")\n",
        "\n",
        "# Inisialisasi dan latih model Word2Vec\n",
        "# parameters:\n",
        "#   sentences: data pelatihan (daftar daftar kata)\n",
        "#   vector_size: jumlah dimensi vektor (misal: 100 dimensi)\n",
        "#   window: ukuran jendela konteks (berapa banyak kata di sekitar yang dilihat)\n",
        "#   min_count: mengabaikan kata yang frekuensinya di bawah ambang batas ini\n",
        "#   workers: jumlah thread CPU untuk pelatihan paralel\n",
        "model_saya = Word2Vec(sentences=korpus_tokenized, vector_size=10, window=2, min_count=1, workers=4)"
      ],
      "metadata": {
        "id": "dkmWxrCwGIDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Akses vektor kata\n",
        "kata_apel = \"apel\"\n",
        "if kata_apel in model_saya.wv: # .wv singkatan dari word vectors\n",
        "    print(f\"\\nVektor untuk '{kata_apel}': {model_saya.wv[kata_apel]}\")\n",
        "else:\n",
        "    print(f\"Kata '{kata_apel}' tidak ditemukan dalam kamus model saya.\")"
      ],
      "metadata": {
        "id": "lBEVNs5CGOpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mencari kata-kata paling mirip\n",
        "kata_target_saya = \"raja\"\n",
        "print(f\"\\nKata-kata paling mirip dengan '{kata_target_saya}':\")\n",
        "if kata_target_saya in model_saya.wv:\n",
        "    similar_words_saya = model_saya.wv.most_similar(kata_target_saya)\n",
        "    for word, similarity in similar_words_saya:\n",
        "        print(f\"- {word}: {similarity:.4f}\")\n",
        "else:\n",
        "    print(f\"Kata '{kata_target_saya}' tidak ditemukan dalam kamus model saya.\")"
      ],
      "metadata": {
        "id": "umC2tvW3GQuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analogi (mungkin tidak terlalu akurat karena korpus kecil)\n",
        "print(\"\\n--- Analogi Kata (Model Saya) ---\")\n",
        "positive_saya = ['raja', 'wanita']\n",
        "negative_saya = ['pria']\n",
        "try:\n",
        "    result_saya = model_saya.wv.most_similar(positive=positive_saya, negative=negative_saya)\n",
        "    for word, similarity in result_saya:\n",
        "        print(f\"- {word}: {similarity:.4f}\")\n",
        "except KeyError as e:\n",
        "    print(f\"Salah satu kata untuk analogi tidak ditemukan: {e}\")"
      ],
      "metadata": {
        "id": "2qkwZZltGSux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Penjelasan Kode Melatih Model Word2Vec Sendiri:\n",
        "1. from gensim.models import Word2Vec: Mengimpor kelas Word2Vec dari gensim.\n",
        "\n",
        "2. korpus_tokenized = [...]: Ini adalah input untuk pelatihan. Ingat, Word2Vec membutuhkan daftar kalimat, di mana setiap kalimat adalah daftar token (kata). Ini adalah hasil dari langkah tokenisasi dan pra-pemrosesan Anda.\n",
        "\n",
        "3. model_saya = Word2Vec(...): Membuat dan melatih model Word2Vec.\n",
        "\n",
        " - sentences: Data teks yang akan digunakan untuk pelatihan.\n",
        "\n",
        " - vector_size: Ukuran dimensi vektor yang akan dihasilkan untuk setiap kata (misalnya, 100). Semakin besar, semakin banyak informasi yang bisa ditampung, tapi butuh lebih banyak data dan komputasi.\n",
        "\n",
        " - window: Jendela konteks. Ini adalah jumlah kata di sebelah kiri dan kanan dari kata target yang akan dipertimbangkan sebagai konteks.\n",
        "\n",
        " - min_count: Jika sebuah kata muncul kurang dari min_count kali di seluruh korpus, kata tersebut akan diabaikan. Ini membantu menyingkirkan kata-kata yang sangat jarang dan mengurangi ukuran kamus.\n",
        "\n",
        " - workers: Jumlah core CPU yang akan digunakan untuk pelatihan paralel.\n",
        "\n",
        "4. model_saya.wv[kata_apel]: .wv adalah objek yang menyimpan semua vektor kata yang sudah dilatih. Anda bisa mengakses vektor kata melalui objek ini.\n",
        "\n",
        "5. model_saya.wv.most_similar(...): Sama seperti model pre-trained, Anda bisa mencari kata-kata yang paling mirip dan melakukan operasi analogi. Catatan: Untuk korpus sekecil ini, hasilnya mungkin tidak sempurna."
      ],
      "metadata": {
        "id": "MdOqNOPPGThP"
      }
    }
  ]
}