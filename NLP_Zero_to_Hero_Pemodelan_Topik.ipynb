{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN4gIC32qMmMJcZDAfrgPPz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/odeandialamsyah/nlp_zero_to_hero/blob/main/NLP_Zero_to_Hero_Pemodelan_Topik.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Untuk Pemodelan Topik, kita akan fokus pada algoritma Latent Dirichlet Allocation (LDA), karena ini adalah yang paling umum dan kuat untuk pengenalan topik. Kita akan menggunakan library Gensim lagi."
      ],
      "metadata": {
        "id": "SP3mNZxDLZwG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Persiapan Data untuk LDA\n",
        "\n",
        "LDA memerlukan input dalam bentuk daftar dokumen yang sudah di-tokenisasi dan biasanya juga sudah pra-proses (lower-case, tanpa stopwords, dll.)."
      ],
      "metadata": {
        "id": "GY94ZLIJKdhe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCkg01TCMLo3",
        "outputId": "54ecb894-7a81-4f2f-c0ea-7587678c4d72"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.3.0.post1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dekzDTdvJoeT"
      },
      "outputs": [],
      "source": [
        "# Import library yang dibutuhkan\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pastikan data NLTK sudah diunduh\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# 1. Contoh Dokumen Mentah\n",
        "dokumen_mentah = [\n",
        "    \"Saya suka makan pizza dan pasta. Makanan Italia sangat lezat.\",\n",
        "    \"Harga saham perusahaan teknologi naik. Investor senang dengan pasar.\",\n",
        "    \"Presiden bertemu dengan menteri untuk membahas kebijakan baru. Politik sedang hangat.\",\n",
        "    \"Anjing dan kucing adalah hewan peliharaan yang lucu. Mereka suka bermain di rumah.\",\n",
        "    \"Saya menikmati pizza dan anjing saya. Apel juga sehat.\"\n",
        "]\n",
        "\n",
        "print(\"Dokumen Mentah:\")\n",
        "for i, doc in enumerate(dokumen_mentah):\n",
        "    print(f\"Dokumen {i+1}: {doc}\")\n",
        "print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhOnQVojK_wE",
        "outputId": "ffa009aa-c529-4209-ef68-e198f5f0187e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dokumen Mentah:\n",
            "Dokumen 1: Saya suka makan pizza dan pasta. Makanan Italia sangat lezat.\n",
            "Dokumen 2: Harga saham perusahaan teknologi naik. Investor senang dengan pasar.\n",
            "Dokumen 3: Presiden bertemu dengan menteri untuk membahas kebijakan baru. Politik sedang hangat.\n",
            "Dokumen 4: Anjing dan kucing adalah hewan peliharaan yang lucu. Mereka suka bermain di rumah.\n",
            "Dokumen 5: Saya menikmati pizza dan anjing saya. Apel juga sehat.\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Pra-pemrosesan Teks (Mengulang langkah yang sudah kita pelajari)\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    # Menghilangkan tanda baca\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    text = text.translate(translator)\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Filter stopwords (gunakan stop_words bahasa Indonesia jika tersedia atau English)\n",
        "    # NLTK memiliki 'indonesian' stopwords, tapi untuk contoh ini saya pakai English\n",
        "    # karena data contoh campur & lebih ilustratif untuk WordNetLemmatizer yang English-centric.\n",
        "    # Untuk data real Indonesia, pastikan pakai list stopwords Bahasa Indonesia.\n",
        "    list_stopwords = set(stopwords.words('indonesian')) # Menggunakan stopwords Indonesia\n",
        "\n",
        "    # Lemmatization (lebih cocok untuk B.Inggris di NLTK, kita lewati untuk B.Indonesia murni)\n",
        "    # Untuk demo ini, kita asumsikan bahasa adalah Inggris untuk fungsi lemmatizer NLTK\n",
        "    # Jika teks Anda murni B.Indonesia, langkah lemmatisasi NLTK ini kurang relevan\n",
        "    # dan Anda perlu library seperti Sastrawi untuk stemming/lemmatisasi B.Indonesia.\n",
        "    # Untuk contoh ini, mari kita asumsikan teks ini sudah \"dilakukan\" stemming/lemmatisasi\n",
        "    # atau kita lewati WordNetLemmatizer untuk menghindari kebingungan jika teks campur.\n",
        "\n",
        "    # Kita akan melakukan filtering stop words saja untuk contoh B.Indonesia ini\n",
        "    filtered_tokens = [word for word in tokens if word not in list_stopwords]\n",
        "    return filtered_tokens\n",
        "\n",
        "dokumen_bersih_tokenized = [preprocess_text(doc) for doc in dokumen_mentah]\n",
        "\n",
        "print(\"Dokumen Bersih (Tokenized):\")\n",
        "for i, doc in enumerate(dokumen_bersih_tokenized):\n",
        "    print(f\"Dokumen {i+1}: {doc}\")\n",
        "print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5SgnaQVLEGv",
        "outputId": "c64c39a5-d911-486c-cde8-e1231ab055c3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dokumen Bersih (Tokenized):\n",
            "Dokumen 1: ['suka', 'makan', 'pizza', 'pasta', 'makanan', 'italia', 'lezat']\n",
            "Dokumen 2: ['harga', 'saham', 'perusahaan', 'teknologi', 'investor', 'senang', 'pasar']\n",
            "Dokumen 3: ['presiden', 'bertemu', 'menteri', 'membahas', 'kebijakan', 'politik', 'hangat']\n",
            "Dokumen 4: ['anjing', 'kucing', 'hewan', 'peliharaan', 'lucu', 'suka', 'bermain', 'rumah']\n",
            "Dokumen 5: ['menikmati', 'pizza', 'anjing', 'apel', 'sehat']\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Membuat Kamus (Dictionary) dari Dokumen yang Sudah di-Tokenisasi\n",
        "# Gensim Dictionary memetakan setiap kata unik ke ID numerik\n",
        "dictionary = corpora.Dictionary(dokumen_bersih_tokenized)\n",
        "print(\"Kamus (Word to ID Mapping):\")\n",
        "# Tampilkan beberapa item dari kamus\n",
        "print(list(dictionary.items())[:10]) # Tampilkan 10 item pertama\n",
        "print(f\"Total kata unik dalam kamus: {len(dictionary)}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ob6yyYmBLHpC",
        "outputId": "5e50cfc7-2a38-4dcd-e245-09e46c6b1d69"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kamus (Word to ID Mapping):\n",
            "[(0, 'italia'), (1, 'lezat'), (2, 'makan'), (3, 'makanan'), (4, 'pasta'), (5, 'pizza'), (6, 'suka'), (7, 'harga'), (8, 'investor'), (9, 'pasar')]\n",
            "Total kata unik dalam kamus: 31\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Membuat Korpus (Bag-of-Words Representasi)\n",
        "# Mengubah setiap dokumen menjadi representasi Bag-of-Words (frekuensi kata)\n",
        "# Ini adalah list of lists of (word_id, word_count) tuples\n",
        "corpus = [dictionary.doc2bow(doc) for doc in dokumen_bersih_tokenized]\n",
        "print(\"Representasi Korpus (Bag-of-Words untuk LDA):\")\n",
        "# Tampilkan representasi BoW untuk dokumen pertama sebagai contoh\n",
        "print(f\"Dokumen 1 BoW: {corpus[0]}\")\n",
        "# Untuk memahami: [(id_kata_pizza, 1), (id_kata_pasta, 1), ...]\n",
        "print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmKq8GT-LKe7",
        "outputId": "154e5041-645e-48ee-988f-302266e4c754"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Representasi Korpus (Bag-of-Words untuk LDA):\n",
            "Dokumen 1 BoW: [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)]\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Melatih Model LDA\n",
        "# num_topics: Jumlah topik yang ingin kita identifikasi\n",
        "# id2word: Kamus yang memetakan ID kembali ke kata\n",
        "# passes: Berapa kali algoritma melewati seluruh korpus\n",
        "# random_state: Untuk memastikan hasil yang bisa direproduksi\n",
        "num_topics = 3 # Kita ingin mencari 3 topik\n",
        "lda_model = LdaModel(corpus=corpus,\n",
        "                     id2word=dictionary,\n",
        "                     num_topics=num_topics,\n",
        "                     passes=15, # Jumlah iterasi pelatihan\n",
        "                     random_state=100)\n",
        "\n",
        "print(\"\\n--- Hasil Pemodelan Topik (LDA) ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXftYEtGLNAR",
        "outputId": "fa72d093-5f0b-43b2-c91f-bfd8bf859251"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Hasil Pemodelan Topik (LDA) ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Menampilkan Topik yang Ditemukan\n",
        "# lda_model.print_topics() akan menampilkan kata-kata kunci untuk setiap topik\n",
        "print(\"Kata-kata kunci untuk setiap Topik:\")\n",
        "for idx, topic in lda_model.print_topics(num_words=5): # Tampilkan 5 kata teratas\n",
        "    print(f\"Topik {idx+1}: {topic}\")\n",
        "print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Orh7s9vWLPuF",
        "outputId": "f2037bb1-7672-4a33-ee63-08fd52107617"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kata-kata kunci untuk setiap Topik:\n",
            "Topik 1: 0.055*\"senang\" + 0.055*\"teknologi\" + 0.055*\"harga\" + 0.055*\"presiden\" + 0.055*\"hangat\"\n",
            "Topik 2: 0.077*\"pizza\" + 0.077*\"suka\" + 0.077*\"italia\" + 0.077*\"pasta\" + 0.077*\"lezat\"\n",
            "Topik 3: 0.100*\"anjing\" + 0.057*\"bermain\" + 0.057*\"peliharaan\" + 0.057*\"hewan\" + 0.057*\"lucu\"\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Menentukan Distribusi Topik untuk Setiap Dokumen\n",
        "print(\"Distribusi Topik per Dokumen:\")\n",
        "for i, doc_bow in enumerate(corpus):\n",
        "    # get_document_topics() akan memberikan distribusi topik untuk dokumen tersebut\n",
        "    doc_topics = lda_model.get_document_topics(doc_bow)\n",
        "    # Urutkan berdasarkan bobot topik tertinggi untuk mudah dibaca\n",
        "    sorted_topics = sorted(doc_topics, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    original_doc = dokumen_mentah[i]\n",
        "    print(f\"Dokumen {i+1}: '{original_doc}'\")\n",
        "    print(f\"   Topik: {[(f'Topik {topic_id+1}', f'{prob:.2f}') for topic_id, prob in sorted_topics]}\")\n",
        "print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aV-m45GnLR3R",
        "outputId": "b880bd63-96e8-4959-a657-97d295312249"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distribusi Topik per Dokumen:\n",
            "Dokumen 1: 'Saya suka makan pizza dan pasta. Makanan Italia sangat lezat.'\n",
            "   Topik: [('Topik 2', '0.91'), ('Topik 3', '0.04'), ('Topik 1', '0.04')]\n",
            "Dokumen 2: 'Harga saham perusahaan teknologi naik. Investor senang dengan pasar.'\n",
            "   Topik: [('Topik 1', '0.92'), ('Topik 2', '0.04'), ('Topik 3', '0.04')]\n",
            "Dokumen 3: 'Presiden bertemu dengan menteri untuk membahas kebijakan baru. Politik sedang hangat.'\n",
            "   Topik: [('Topik 1', '0.92'), ('Topik 2', '0.04'), ('Topik 3', '0.04')]\n",
            "Dokumen 4: 'Anjing dan kucing adalah hewan peliharaan yang lucu. Mereka suka bermain di rumah.'\n",
            "   Topik: [('Topik 3', '0.92'), ('Topik 2', '0.04'), ('Topik 1', '0.04')]\n",
            "Dokumen 5: 'Saya menikmati pizza dan anjing saya. Apel juga sehat.'\n",
            "   Topik: [('Topik 3', '0.89'), ('Topik 2', '0.06'), ('Topik 1', '0.06')]\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Penjelasan Kode Pemodelan Topik (LDA):\n",
        "1. Pra-pemrosesan (ulang): Kita mengulang langkah pra-pemrosesan yang sudah Anda kuasai. Penting untuk memiliki teks yang bersih dan di-tokenisasi sebelum membangun model topik. Di sini, kita menggunakan stopwords Bahasa Indonesia. Perlu dicatat bahwa WordNetLemmatizer dari NLTK paling efektif untuk Bahasa Inggris. Jika korpus Anda murni Bahasa Indonesia, Anda akan memerlukan library stemming/lemmatization Bahasa Indonesia seperti Sastrawi atau InStem. Untuk contoh di atas, saya hanya menerapkan lowercase dan penghilangan stopwords (Indonesia) saja untuk kesederhanaan.\n",
        "\n",
        "2. dictionary=corpora.Dictionary(dokumen_bersih_tokenized): Ini adalah langkah penting Gensim. Ia memindai semua dokumen yang sudah di-tokenisasi dan membangun sebuah kamus (vocabulary). Setiap kata unik di korpus diberi ID numerik.\n",
        "\n",
        "3. corpus = [dictionary.doc2bow(doc) for doc in dokumen_bersih_tokenized]: Setelah kamus dibuat, setiap dokumen diubah menjadi representasi Bag-of-Words (BoW) dalam format Gensim. Ini adalah daftar tuple (word_id, word_count). LDA menerima input dalam format ini.\n",
        "\n",
        "4. lda_model = LdaModel(...): Inisialisasi dan melatih model LDA.\n",
        "\n",
        " - corpus: Data BoW yang kita buat.\n",
        "\n",
        " - id2word: Kamus yang dibuat sebelumnya. Ini digunakan model untuk memetakan ID kembali ke kata saat menampilkan topik.\n",
        "\n",
        " - num_topics: Ini adalah parameter krusial. Anda perlu menentukan berapa banyak topik yang ingin Anda \"temukan\". Pilihan ini seringkali berdasarkan domain pengetahuan atau melalui evaluasi performa model.\n",
        "\n",
        " - passes: Jumlah kali algoritma LDA mengiterasi seluruh korpus. Semakin banyak passes, semakin baik model belajar, tapi semakin lama.\n",
        "\n",
        " - random_state: Untuk memastikan hasil yang sama setiap kali Anda menjalankan kode.\n",
        "\n",
        "6. lda_model.print_topics(num_words=5): Setelah model dilatih, Anda bisa melihat topik-topik yang ditemukan. Setiap topik direpresentasikan oleh daftar kata-kata yang paling mungkin muncul dalam topik tersebut, bersama dengan bobotnya. Ini membantu Anda menginterpretasikan topik.\n",
        "\n",
        "7. lda_model.get_document_topics(doc_bow): Untuk setiap dokumen, Anda bisa mendapatkan distribusi topik. Ini menunjukkan topik mana saja yang paling dominan dalam dokumen tersebut beserta probabilitasnya.\n",
        "\n",
        "Anda bisa mencoba mengubah num_topics dan melihat bagaimana kata-kata kunci dalam setiap topik berubah. Semakin banyak data yang Anda miliki, semakin jelas dan koheren topik-topik yang akan ditemukan oleh LDA."
      ],
      "metadata": {
        "id": "_6xAvcwQXLjs"
      }
    }
  ]
}