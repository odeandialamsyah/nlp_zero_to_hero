{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMGGmpjEXdAbxgj0cApirTE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/odeandialamsyah/nlp_zero_to_hero/blob/main/NLP_Zero_to_Hero_Representasi_Teks_Sederhana.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Kode CountVectorizer"
      ],
      "metadata": {
        "id": "QYeaEYSGClKU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4uG9S0zK_NLi"
      },
      "outputs": [],
      "source": [
        "# Import library yang dibutuhkan\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Contoh dokumen (setelah pra-pemrosesan awal, kita anggap sudah bersih)\n",
        "# Dalam praktik nyata, Anda akan menggunakan output dari langkah pra-pemrosesan sebelumnya\n",
        "dokumen = [\n",
        "    \"saya suka makan apel\",\n",
        "    \"saya suka pisang dan apel\",\n",
        "    \"kucing suka ikan\",\n",
        "    \"anjing suka bermain\"\n",
        "]\n",
        "\n",
        "print(f\"Dokumen Asli (setelah pra-pemrosesan): {dokumen}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wzHqHp9_1z6",
        "outputId": "63de34f8-75b7-48d7-8bec-458b8ce7820e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dokumen Asli (setelah pra-pemrosesan): ['saya suka makan apel', 'saya suka pisang dan apel', 'kucing suka ikan', 'anjing suka bermain']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Membuat Model Bag-of-Words ---\n",
        "# Inisialisasi CountVectorizer\n",
        "# stop_words='english' bisa digunakan jika teksnya B.Inggris dan kita ingin menghapus stopwords otomatis.\n",
        "# analyzer='word' adalah default, berarti token adalah kata.\n",
        "# Contoh ini mengasumsikan dokumen sudah bersih dari stopwords\n",
        "vectorizer_bow = CountVectorizer()"
      ],
      "metadata": {
        "id": "kmZqUhjH_413"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitur learning dan transformasi data\n",
        "# fit_transform() akan:\n",
        "# 1. Membangun kamus dari semua kata unik di 'dokumen'.\n",
        "# 2. Menghitung frekuensi setiap kata untuk setiap dokumen.\n",
        "bow_matrix = vectorizer_bow.fit_transform(dokumen)\n",
        "\n",
        "# Mendapatkan nama fitur (yaitu, kata-kata dalam kamus)\n",
        "feature_names_bow = vectorizer_bow.get_feature_names_out()\n",
        "\n",
        "print(\"1. Representasi Bag-of-Words (BoW):\")\n",
        "print(\"   Kamus (Vocabulary):\", feature_names_bow)\n",
        "print(\"   Matriks BoW (Frekuensi Kata):\")\n",
        "print(bow_matrix.toarray()) # .toarray() untuk melihat representasi dense\n",
        "print(\"\\n\")\n",
        "\n",
        "# Mari kita coba memahami output untuk dokumen pertama: \"saya suka makan apel\"\n",
        "# Berdasarkan kamus: ['anjing' 'apel' 'bermain' 'dan' 'ikan' 'kucing' 'makan' 'pisang' 'saya' 'suka']\n",
        "# Dokumen pertama: \"saya suka makan apel\"\n",
        "# Frekuensi: anjing:0, apel:1, bermain:0, dan:0, ikan:0, kucing:0, makan:1, pisang:0, saya:1, suka:1\n",
        "# Hasilnya: [0 1 0 0 0 0 1 0 1 1] (sesuai urutan kamus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgwxFNBT_8It",
        "outputId": "09ccddc0-691c-4938-a74a-a7088b5f29fc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Representasi Bag-of-Words (BoW):\n",
            "   Kamus (Vocabulary): ['anjing' 'apel' 'bermain' 'dan' 'ikan' 'kucing' 'makan' 'pisang' 'saya'\n",
            " 'suka']\n",
            "   Matriks BoW (Frekuensi Kata):\n",
            "[[0 1 0 0 0 0 1 0 1 1]\n",
            " [0 1 0 1 0 0 0 1 1 1]\n",
            " [0 0 0 0 1 1 0 0 0 1]\n",
            " [1 0 1 0 0 0 0 0 0 1]]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Penjelasan Kode CountVectorizer:\n",
        "\n",
        "1. from sklearn.feature_extraction.text import CountVectorizer: Mengimpor kelas CountVectorizer dari modul feature_extraction.text di scikit-learn.\n",
        "\n",
        "2. dokumen = [...]: Ini adalah kumpulan teks kita. Untuk kesederhanaan, kita langsung menggunakan teks yang sudah diasumsikan bersih (tanpa tanda baca, lowercase, dll.). Dalam proyek nyata, output dari langkah pra-pemrosesan sebelumnya akan menjadi input di sini.\n",
        "\n",
        "3. vectorizer_bow = CountVectorizer(): Membuat sebuah objek CountVectorizer. Objek ini akan belajar kamus dari data dan kemudian mengubah teks menjadi vektor frekuensi kata.\n",
        "\n",
        "4. bow_matrix = vectorizer_bow.fit_transform(dokumen): Ini adalah langkah kunci.\n",
        "\n",
        "5. fit(): Membangun kamus unik dari semua kata yang ada dalam daftar dokumen. Setiap kata unik akan diberi indeks.\n",
        "\n",
        "6. transform(): Mengubah setiap dokumen menjadi vektor numerik berdasarkan kamus yang sudah dibuat. Setiap posisi dalam vektor merepresentasikan sebuah kata dari kamus, dan nilainya adalah frekuensi kemunculan kata tersebut di dokumen.\n",
        "\n",
        "7. Hasilnya, bow_matrix adalah matriks (seringkali sparse matrix untuk efisiensi) di mana barisnya adalah dokumen dan kolomnya adalah kata-kata dari kamus.\n",
        "\n",
        "8. feature_names_bow = vectorizer_bow.get_feature_names_out(): Mengambil daftar kata-kata yang ada di kamus (vocabulary) yang dibangun oleh CountVectorizer. Ini penting untuk memahami kolom-kolom di bow_matrix.\n",
        "\n",
        "10. print(bow_matrix.toarray()): CountVectorizer secara default menghasilkan sparse matrix (matriks yang sebagian besar nilainya nol) untuk menghemat memori. .toarray() mengubahnya menjadi dense array (array biasa) agar lebih mudah dilihat dan dipahami di konsol."
      ],
      "metadata": {
        "id": "pA77ro21BfFM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Kode TfidfVectorizer"
      ],
      "metadata": {
        "id": "HzIyxCwkCrAg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import library yang dibutuhkan\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "print(f\"Dokumen Asli (setelah pra-pemrosesan): {dokumen}\\n\")\n",
        "\n",
        "# --- Membuat Model TF-IDF ---\n",
        "# Inisialisasi TfidfVectorizer\n",
        "# Sama seperti CountVectorizer, ini bisa mengurus tokenisasi dan stopwords jika diperlukan.\n",
        "# Namun, kita asumsikan teks sudah bersih di sini.\n",
        "vectorizer_tfidf = TfidfVectorizer()\n",
        "\n",
        "# Fitur learning dan transformasi data\n",
        "tfidf_matrix = vectorizer_tfidf.fit_transform(dokumen)\n",
        "\n",
        "# Mendapatkan nama fitur (kata-kata dalam kamus)\n",
        "feature_names_tfidf = vectorizer_tfidf.get_feature_names_out()\n",
        "\n",
        "print(\"2. Representasi TF-IDF:\")\n",
        "print(\"   Kamus (Vocabulary):\", feature_names_tfidf)\n",
        "print(\"   Matriks TF-IDF:\")\n",
        "print(tfidf_matrix.toarray()) # .toarray() untuk melihat representasi dense\n",
        "print(\"\\n\")\n",
        "\n",
        "# Mari kita analisis sebentar:\n",
        "# Perhatikan bahwa kata-kata umum seperti 'saya' dan 'suka' memiliki nilai TF-IDF yang sangat rendah atau bahkan nol (tergantung pembulatan).\n",
        "# Sedangkan kata-kata spesifik seperti 'makan', 'pisang', 'kucing', 'anjing', 'ikan', 'bermain' memiliki nilai yang lebih tinggi.\n",
        "# Ini adalah bukti bahwa TF-IDF memberikan bobot lebih pada kata-kata yang lebih informatif/spesifik!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbY0YDENB7LO",
        "outputId": "a527950d-f2d1-48f2-fd9d-44e0e4c81fd4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dokumen Asli (setelah pra-pemrosesan): ['saya suka makan apel', 'saya suka pisang dan apel', 'kucing suka ikan', 'anjing suka bermain']\n",
            "\n",
            "2. Representasi TF-IDF:\n",
            "   Kamus (Vocabulary): ['anjing' 'apel' 'bermain' 'dan' 'ikan' 'kucing' 'makan' 'pisang' 'saya'\n",
            " 'suka']\n",
            "   Matriks TF-IDF:\n",
            "[[0.         0.4970962  0.         0.         0.         0.\n",
            "  0.6305035  0.         0.4970962  0.32902288]\n",
            " [0.         0.42049337 0.         0.53334252 0.         0.\n",
            "  0.         0.53334252 0.42049337 0.27832025]\n",
            " [0.         0.         0.         0.         0.66338461 0.66338461\n",
            "  0.         0.         0.         0.34618161]\n",
            " [0.66338461 0.         0.66338461 0.         0.         0.\n",
            "  0.         0.         0.         0.34618161]]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Penjelasan Kode TfidfVectorizer:\n",
        "1. from sklearn.feature_extraction.text import TfidfVectorizer: Mengimpor kelas TfidfVectorizer.\n",
        "\n",
        "2. vectorizer_tfidf = TfidfVectorizer(): Membuat objek TfidfVectorizer. Objek ini akan menghitung TF dan IDF untuk setiap kata di setiap dokumen.\n",
        "\n",
        "3. tfidf_matrix = vectorizer_tfidf.fit_transform(dokumen): Mirip dengan CountVectorizer, langkah ini membangun kamus dan kemudian mengubah setiap dokumen menjadi vektor nilai TF-IDF.\n",
        "\n",
        "4. feature_names_tfidf = vectorizer_tfidf.get_feature_names_out(): Mengambil daftar kata-kata yang ada di kamus TF-IDF.\n",
        "\n",
        "5. print(tfidf_matrix.toarray()): Menampilkan matriks TF-IDF dalam bentuk dense array.\n",
        "\n"
      ],
      "metadata": {
        "id": "nvn5In-GCI4M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eksplorasi & Latihan Anda:\n",
        "1. Jalankan kedua bagian kode (BoW dan TF-IDF) secara berurutan. Bandingkan output matriksnya.\n",
        "\n",
        "2. Tambahkan dokumen baru ke daftar dokumen Anda. Perhatikan bagaimana kamus (feature_names) bertambah dan bagaimana matriks bow_matrix dan tfidf_matrix berubah.\n",
        "\n",
        "3. Coba analisis nilai-nilai TF-IDF untuk kata-kata umum (seperti \"saya\", \"suka\") dan kata-kata yang lebih spesifik (seperti \"kucing\", \"ikan\"). Apakah Anda melihat tren seperti yang kita diskusikan? (Kata umum bobotnya rendah, kata spesifik bobotnya tinggi)."
      ],
      "metadata": {
        "id": "9GWWO13bCYg3"
      }
    }
  ]
}